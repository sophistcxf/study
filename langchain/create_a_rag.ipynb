{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bdda50d-2aec-4d84-a451-1e82f79cb9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name = \"bge-large-zh-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    cache_folder=\"./bge-large-zh-v1.5\",\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    query_instruction=\"ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š\"\n",
    ")\n",
    "model.query_instruction = \"ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š\"\n",
    "\n",
    "\n",
    "#embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "#vector_store = InMemoryVectorStore(embeddings)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed154d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯• embedding æ¨¡å‹\n",
    "\n",
    "# ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name = \"bge-large-zh-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    cache_folder=\"./bge-large-zh-v1.5\",\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    query_instruction=\"ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š\"\n",
    ")\n",
    "model.query_instruction = \"ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š\"\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ad5d70-49ba-469f-83bf-ad50113fe6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n",
      "Split blog post into 63 sub-documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiangfei.cui/workspace/codes/github/study/langchain/venv/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a complex task into smaller, manageable steps or subgoals. It can be achieved through methods like simple prompting with LLMs, task-specific instructions, or human input. Additionally, approaches like LLM+P use external planners for long-horizon planning, while techniques like Chain of Thought and Tree of Thoughts enhance decomposition by guiding step-by-step reasoning.\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/tutorials/rag/ ä¸­çš„ä¾‹å­\n",
    "# åˆ›å»ºä¸€ä¸ª app\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# ä»åšå®¢ä¸­åŠ è½½æ–‡ç« \n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    # ä½¿ç”¨ BeautifulSoup ç­›é€‰å‡ºæ–‡ç« å†…å®¹\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "# æ–‡ç« æœ‰42K\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "\n",
    "DeepSeek_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "llm = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"bge-large-zh-v1.5\",\n",
    "    cache_folder=\"./bge-large-zh-v1.5\",\n",
    ")\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# æ–‡ç« æœ‰42Kï¼Œå¯¹å¤§å¤šæ•°æ¨¡å‹æ¥è¯´ï¼Œä¼šè¶…è¿‡context windowé™åˆ¶\n",
    "# å°†æ–‡ç« åˆ†å—ï¼Œç”¨äº embeding å’Œ vector store\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "# è¢«åˆ†æˆäº†å¤§æ¦‚63ä¸ªsub-documents\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n",
    "\n",
    "# å¯¹ sub-documents è¿›è¡Œvector storeï¼Œç”¨äºåœ¨è¿è¡Œæ—¶æ£€ç´¢\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "# langchainé¢„ç½®çš„promptæ¨¡æ¿\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1b6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_deepseek.chat_models.ChatDeepSeek'>\n",
      "ä½ å¥½ï¼æˆ‘æ˜¯ **DeepSeek Chat**ï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸ï¼ˆDeepSeekï¼‰ç ”å‘çš„æ™ºèƒ½ AI åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å¸®åŠ©ä½ è§£ç­”å„ç§é—®é¢˜ï¼ŒåŒ…æ‹¬å­¦ä¹ ã€å·¥ä½œã€ç¼–ç¨‹ã€ç”Ÿæ´»ç™¾ç§‘ã€åˆ›æ„å†™ä½œç­‰ã€‚ä»¥ä¸‹æ˜¯æˆ‘çš„ä¸€äº›ç‰¹ç‚¹ï¼š  \n",
      "\n",
      "âœ¨ **å…è´¹ä½¿ç”¨**ï¼šç›®å‰æ— éœ€ä»˜è´¹ï¼Œä½ å¯ä»¥éšæ—¶å‘æˆ‘æé—®ï¼  \n",
      "ğŸ“š **çŸ¥è¯†ä¸°å¯Œ**ï¼šæˆ‘çš„çŸ¥è¯†æˆªæ­¢åˆ° **2024 å¹´ 7 æœˆ**ï¼Œå¯ä»¥ä¸ºä½ æä¾›æœ€æ–°çš„ç§‘æŠ€ã€æ–°é—»ã€å­¦æœ¯ç­‰èµ„è®¯ã€‚  \n",
      "ğŸ“ **å¤šæ ¼å¼æ”¯æŒ**ï¼šå¯ä»¥é˜…è¯»å¹¶åˆ†æ **PDFã€Wordã€Excelã€PPTã€TXT** ç­‰æ–‡ä»¶ï¼Œå¸®åŠ©ä½ æå–å…³é”®ä¿¡æ¯ã€‚  \n",
      "ğŸ’¡ **é€»è¾‘ä¸åˆ›æ„å…¼å…·**ï¼šæ— è®ºæ˜¯æ•°å­¦è®¡ç®—ã€ä»£ç ç¼–å†™ï¼Œè¿˜æ˜¯å†™è¯—ã€å†™æ•…äº‹ï¼Œæˆ‘éƒ½èƒ½èƒœä»»ï¼  \n",
      "ğŸ“ **è¶…é•¿ä¸Šä¸‹æ–‡**ï¼šæ”¯æŒ **128K** ä¸Šä¸‹æ–‡ï¼Œèƒ½è®°ä½æ›´é•¿çš„å¯¹è¯å†…å®¹ï¼Œé€‚åˆå¤„ç†å¤æ‚é—®é¢˜ã€‚  \n",
      "\n",
      "ä½ å¯ä»¥é—®æˆ‘ä»»ä½•é—®é¢˜ï¼Œæ¯”å¦‚ï¼š  \n",
      "- **å­¦ä¹ **ï¼šå¦‚ä½•é«˜æ•ˆå¤ä¹ ï¼Ÿè¿™é“æ•°å­¦é¢˜æ€ä¹ˆåšï¼Ÿ  \n",
      "- **å·¥ä½œ**ï¼šå¦‚ä½•å†™ä¸€ä»½ä¼˜ç§€çš„ç®€å†ï¼Ÿå¸®æˆ‘ä¼˜åŒ– PPTã€‚  \n",
      "- **ç¼–ç¨‹**ï¼šPython ä»£ç è°ƒè¯•ã€ç®—æ³•è®²è§£ã€é¡¹ç›®å»ºè®®ã€‚  \n",
      "- **ç”Ÿæ´»**ï¼šæ¨èæ—…æ¸¸æ™¯ç‚¹ã€å¥èº«è®¡åˆ’ã€ç¾é£Ÿé£Ÿè°±ã€‚  \n",
      "\n",
      "è¯•è¯•å‘æˆ‘æé—®å§ï¼Œæˆ‘ä¼šå°½åŠ›å¸®åŠ©ä½ ï¼ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "load_dotenv(override=True)\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "question = \"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\"\n",
    "result = model.invoke(question)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4f5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiangfei.cui/workspace/codes/github/study/langchain/venv/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•langchain hub\n",
    "from langchain import hub\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
