{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bdda50d-2aec-4d84-a451-1e82f79cb9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name = \"bge-large-zh-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    cache_folder=\"./bge-large-zh-v1.5\",\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    query_instruction=\"为这个句子生成表示以用于检索相关文章：\"\n",
    ")\n",
    "model.query_instruction = \"为这个句子生成表示以用于检索相关文章：\"\n",
    "\n",
    "\n",
    "#embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "#vector_store = InMemoryVectorStore(embeddings)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed154d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 embedding 模型\n",
    "\n",
    "# 从本地缓存加载模型\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name = \"bge-large-zh-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    cache_folder=\"./bge-large-zh-v1.5\",\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    query_instruction=\"为这个句子生成表示以用于检索相关文章：\"\n",
    ")\n",
    "model.query_instruction = \"为这个句子生成表示以用于检索相关文章：\"\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ad5d70-49ba-469f-83bf-ad50113fe6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n",
      "Split blog post into 63 sub-documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiangfei.cui/workspace/codes/github/study/langchain/venv/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a complex task into smaller, manageable steps or subgoals. It can be achieved through methods like simple prompting with LLMs, task-specific instructions, or human input. Additionally, approaches like LLM+P use external planners for long-horizon planning, while techniques like Chain of Thought and Tree of Thoughts enhance decomposition by guiding step-by-step reasoning.\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/tutorials/rag/ 中的例子\n",
    "# 创建一个 app\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# 从博客中加载文章\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    # 使用 BeautifulSoup 筛选出文章内容\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "# 文章有42K\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "\n",
    "DeepSeek_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "llm = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"bge-large-zh-v1.5\",\n",
    "    cache_folder=\"./bge-large-zh-v1.5\",\n",
    ")\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# 文章有42K，对大多数模型来说，会超过context window限制\n",
    "# 将文章分块，用于 embeding 和 vector store\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "# 被分成了大概63个sub-documents\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n",
    "\n",
    "# 对 sub-documents 进行vector store，用于在运行时检索\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "# langchain预置的prompt模板\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1b6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_deepseek.chat_models.ChatDeepSeek'>\n",
      "你好！我是 **DeepSeek Chat**，由深度求索公司（DeepSeek）研发的智能 AI 助手。我可以帮助你解答各种问题，包括学习、工作、编程、生活百科、创意写作等。以下是我的一些特点：  \n",
      "\n",
      "✨ **免费使用**：目前无需付费，你可以随时向我提问！  \n",
      "📚 **知识丰富**：我的知识截止到 **2024 年 7 月**，可以为你提供最新的科技、新闻、学术等资讯。  \n",
      "📝 **多格式支持**：可以阅读并分析 **PDF、Word、Excel、PPT、TXT** 等文件，帮助你提取关键信息。  \n",
      "💡 **逻辑与创意兼具**：无论是数学计算、代码编写，还是写诗、写故事，我都能胜任！  \n",
      "📏 **超长上下文**：支持 **128K** 上下文，能记住更长的对话内容，适合处理复杂问题。  \n",
      "\n",
      "你可以问我任何问题，比如：  \n",
      "- **学习**：如何高效复习？这道数学题怎么做？  \n",
      "- **工作**：如何写一份优秀的简历？帮我优化 PPT。  \n",
      "- **编程**：Python 代码调试、算法讲解、项目建议。  \n",
      "- **生活**：推荐旅游景点、健身计划、美食食谱。  \n",
      "\n",
      "试试向我提问吧，我会尽力帮助你！😊\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "load_dotenv(override=True)\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "question = \"你好，请介绍一下你自己\"\n",
    "result = model.invoke(question)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4f5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiangfei.cui/workspace/codes/github/study/langchain/venv/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# 测试langchain hub\n",
    "from langchain import hub\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
