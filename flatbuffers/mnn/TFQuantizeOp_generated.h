// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_TFQUANTIZEOP_MNN_H_
#define FLATBUFFERS_GENERATED_TFQUANTIZEOP_MNN_H_

#include "flatbuffers/flatbuffers.h"

// Ensure the included flatbuffers.h is the same version as when this file was
// generated, otherwise it may not be compatible.
static_assert(FLATBUFFERS_VERSION_MAJOR == 25 &&
              FLATBUFFERS_VERSION_MINOR == 2 &&
              FLATBUFFERS_VERSION_REVISION == 10,
             "Non-compatible flatbuffers version included");

#include "CaffeOp_generated.h"
#include "Tensor_generated.h"
#include "Type_generated.h"

namespace MNN {

struct QuantizedParam;
struct QuantizedParamBuilder;

struct QuantizedAdd;
struct QuantizedAddBuilder;

struct Dequantize;
struct DequantizeBuilder;

struct QuantizedAvgPool;
struct QuantizedAvgPoolBuilder;

struct QuantizedBiasAdd;
struct QuantizedBiasAddBuilder;

struct QuantizedConcat;
struct QuantizedConcatBuilder;

struct QuantizedLogistic;
struct QuantizedLogisticBuilder;

struct QuantizedMatMul;
struct QuantizedMatMulBuilder;

struct QuantizedMaxPool;
struct QuantizedMaxPoolBuilder;

struct QuantizedRelu;
struct QuantizedReluBuilder;

struct QuantizedRelu6;
struct QuantizedRelu6Builder;

struct QuantizedReshape;
struct QuantizedReshapeBuilder;

struct QuantizedSoftmax;
struct QuantizedSoftmaxBuilder;

struct QuantizeV2;
struct QuantizeV2Builder;

struct RequantizationRange;
struct RequantizationRangeBuilder;

struct Requantize;
struct RequantizeBuilder;

struct TfQuantizedConv2D;
struct TfQuantizedConv2DBuilder;

enum FusedActivation : int8_t {
  FusedActivation_kTfLiteActNone = 0,
  FusedActivation_kTfLiteActRelu = 1,
  FusedActivation_kTfLiteActRelu1 = 2,
  FusedActivation_kTfLiteActRelu6 = 3,
  FusedActivation_kTfLiteActTanh = 4,
  FusedActivation_kTfLiteActSignBit = 5,
  FusedActivation_kTfLiteActSigmoid = 6,
  FusedActivation_MIN = FusedActivation_kTfLiteActNone,
  FusedActivation_MAX = FusedActivation_kTfLiteActSigmoid
};

inline const FusedActivation (&EnumValuesFusedActivation())[7] {
  static const FusedActivation values[] = {
    FusedActivation_kTfLiteActNone,
    FusedActivation_kTfLiteActRelu,
    FusedActivation_kTfLiteActRelu1,
    FusedActivation_kTfLiteActRelu6,
    FusedActivation_kTfLiteActTanh,
    FusedActivation_kTfLiteActSignBit,
    FusedActivation_kTfLiteActSigmoid
  };
  return values;
}

inline const char * const *EnumNamesFusedActivation() {
  static const char * const names[8] = {
    "kTfLiteActNone",
    "kTfLiteActRelu",
    "kTfLiteActRelu1",
    "kTfLiteActRelu6",
    "kTfLiteActTanh",
    "kTfLiteActSignBit",
    "kTfLiteActSigmoid",
    nullptr
  };
  return names;
}

inline const char *EnumNameFusedActivation(FusedActivation e) {
  if (::flatbuffers::IsOutRange(e, FusedActivation_kTfLiteActNone, FusedActivation_kTfLiteActSigmoid)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesFusedActivation()[index];
}

enum ModeFormat : int8_t {
  ModeFormat_TENSORFLOW = 0,
  ModeFormat_TFLITE = 1,
  ModeFormat_MIN = ModeFormat_TENSORFLOW,
  ModeFormat_MAX = ModeFormat_TFLITE
};

inline const ModeFormat (&EnumValuesModeFormat())[2] {
  static const ModeFormat values[] = {
    ModeFormat_TENSORFLOW,
    ModeFormat_TFLITE
  };
  return values;
}

inline const char * const *EnumNamesModeFormat() {
  static const char * const names[3] = {
    "TENSORFLOW",
    "TFLITE",
    nullptr
  };
  return names;
}

inline const char *EnumNameModeFormat(ModeFormat e) {
  if (::flatbuffers::IsOutRange(e, ModeFormat_TENSORFLOW, ModeFormat_TFLITE)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesModeFormat()[index];
}

enum QuantizeMode : int8_t {
  QuantizeMode_MIN_COMBINED = 0,
  QuantizeMode_MIN_FIRST = 1,
  QuantizeMode_SCALED = 2,
  QuantizeMode_MIN = QuantizeMode_MIN_COMBINED,
  QuantizeMode_MAX = QuantizeMode_SCALED
};

inline const QuantizeMode (&EnumValuesQuantizeMode())[3] {
  static const QuantizeMode values[] = {
    QuantizeMode_MIN_COMBINED,
    QuantizeMode_MIN_FIRST,
    QuantizeMode_SCALED
  };
  return values;
}

inline const char * const *EnumNamesQuantizeMode() {
  static const char * const names[4] = {
    "MIN_COMBINED",
    "MIN_FIRST",
    "SCALED",
    nullptr
  };
  return names;
}

inline const char *EnumNameQuantizeMode(QuantizeMode e) {
  if (::flatbuffers::IsOutRange(e, QuantizeMode_MIN_COMBINED, QuantizeMode_SCALED)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesQuantizeMode()[index];
}

enum QuantizeRoundMode : int8_t {
  QuantizeRoundMode_HALF_AWAY_FROM_ZERO = 0,
  QuantizeRoundMode_HALF_TO_EVEN = 1,
  QuantizeRoundMode_MIN = QuantizeRoundMode_HALF_AWAY_FROM_ZERO,
  QuantizeRoundMode_MAX = QuantizeRoundMode_HALF_TO_EVEN
};

inline const QuantizeRoundMode (&EnumValuesQuantizeRoundMode())[2] {
  static const QuantizeRoundMode values[] = {
    QuantizeRoundMode_HALF_AWAY_FROM_ZERO,
    QuantizeRoundMode_HALF_TO_EVEN
  };
  return values;
}

inline const char * const *EnumNamesQuantizeRoundMode() {
  static const char * const names[3] = {
    "HALF_AWAY_FROM_ZERO",
    "HALF_TO_EVEN",
    nullptr
  };
  return names;
}

inline const char *EnumNameQuantizeRoundMode(QuantizeRoundMode e) {
  if (::flatbuffers::IsOutRange(e, QuantizeRoundMode_HALF_AWAY_FROM_ZERO, QuantizeRoundMode_HALF_TO_EVEN)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesQuantizeRoundMode()[index];
}

struct QuantizedParam FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef QuantizedParamBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ZEROPOINT = 4,
    VT_SCALE = 6
  };
  int32_t zeroPoint() const {
    return GetField<int32_t>(VT_ZEROPOINT, 0);
  }
  float scale() const {
    return GetField<float>(VT_SCALE, 0.0f);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_ZEROPOINT, 4) &&
           VerifyField<float>(verifier, VT_SCALE, 4) &&
           verifier.EndTable();
  }
};

struct QuantizedParamBuilder {
  typedef QuantizedParam Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_zeroPoint(int32_t zeroPoint) {
    fbb_.AddElement<int32_t>(QuantizedParam::VT_ZEROPOINT, zeroPoint, 0);
  }
  void add_scale(float scale) {
    fbb_.AddElement<float>(QuantizedParam::VT_SCALE, scale, 0.0f);
  }
  explicit QuantizedParamBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<QuantizedParam> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<QuantizedParam>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<QuantizedParam> CreateQuantizedParam(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int32_t zeroPoint = 0,
    float scale = 0.0f) {
  QuantizedParamBuilder builder_(_fbb);
  builder_.add_scale(scale);
  builder_.add_zeroPoint(zeroPoint);
  return builder_.Finish();
}

struct QuantizedAdd FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef QuantizedAddBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ACTIVATIONTYPE = 4,
    VT_INPUT1QUANTIZEDPARAM = 6,
    VT_INPUT2QUANTIZEDPARAM = 8,
    VT_OUTPUTQUANTIZEDPARAM = 10
  };
  MNN::FusedActivation activationType() const {
    return static_cast<MNN::FusedActivation>(GetField<int8_t>(VT_ACTIVATIONTYPE, 0));
  }
  const MNN::QuantizedParam *input1QuantizedParam() const {
    return GetPointer<const MNN::QuantizedParam *>(VT_INPUT1QUANTIZEDPARAM);
  }
  const MNN::QuantizedParam *input2QuantizedParam() const {
    return GetPointer<const MNN::QuantizedParam *>(VT_INPUT2QUANTIZEDPARAM);
  }
  const MNN::QuantizedParam *outputQuantizedParam() const {
    return GetPointer<const MNN::QuantizedParam *>(VT_OUTPUTQUANTIZEDPARAM);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int8_t>(verifier, VT_ACTIVATIONTYPE, 1) &&
           VerifyOffset(verifier, VT_INPUT1QUANTIZEDPARAM) &&
           verifier.VerifyTable(input1QuantizedParam()) &&
           VerifyOffset(verifier, VT_INPUT2QUANTIZEDPARAM) &&
           verifier.VerifyTable(input2QuantizedParam()) &&
           VerifyOffset(verifier, VT_OUTPUTQUANTIZEDPARAM) &&
           verifier.VerifyTable(outputQuantizedParam()) &&
           verifier.EndTable();
  }
};

struct QuantizedAddBuilder {
  typedef QuantizedAdd Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_activationType(MNN::FusedActivation activationType) {
    fbb_.AddElement<int8_t>(QuantizedAdd::VT_ACTIVATIONTYPE, static_cast<int8_t>(activationType), 0);
  }
  void add_input1QuantizedParam(::flatbuffers::Offset<MNN::QuantizedParam> input1QuantizedParam) {
    fbb_.AddOffset(QuantizedAdd::VT_INPUT1QUANTIZEDPARAM, input1QuantizedParam);
  }
  void add_input2QuantizedParam(::flatbuffers::Offset<MNN::QuantizedParam> input2QuantizedParam) {
    fbb_.AddOffset(QuantizedAdd::VT_INPUT2QUANTIZEDPARAM, input2QuantizedParam);
  }
  void add_outputQuantizedParam(::flatbuffers::Offset<MNN::QuantizedParam> outputQuantizedParam) {
    fbb_.AddOffset(QuantizedAdd::VT_OUTPUTQUANTIZEDPARAM, outputQuantizedParam);
  }
  explicit QuantizedAddBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<QuantizedAdd> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<QuantizedAdd>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<QuantizedAdd> CreateQuantizedAdd(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    MNN::FusedActivation activationType = MNN::FusedActivation_kTfLiteActNone,
    ::flatbuffers::Offset<MNN::QuantizedParam> input1QuantizedParam = 0,
    ::flatbuffers::Offset<MNN::QuantizedParam> input2QuantizedParam = 0,
    ::flatbuffers::Offset<MNN::QuantizedParam> outputQuantizedParam = 0) {
  QuantizedAddBuilder builder_(_fbb);
  builder_.add_outputQuantizedParam(outputQuantizedParam);
  builder_.add_input2QuantizedParam(input2QuantizedParam);
  builder_.add_input1QuantizedParam(input1QuantizedParam);
  builder_.add_activationType(activationType);
  return builder_.Finish();
}

struct Dequantize FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef DequantizeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INPUTQUANTIZEDPARAM = 4,
    VT_MODE = 6,
    VT_MODELFORMAT = 8,
    VT_TYPE = 10
  };
  const MNN::QuantizedParam *inputQuantizedParam() const {
    return GetPointer<const MNN::QuantizedParam *>(VT_INPUTQUANTIZEDPARAM);
  }
  MNN::QuantizeMode mode() const {
    return static_cast<MNN::QuantizeMode>(GetField<int8_t>(VT_MODE, 0));
  }
  MNN::ModeFormat modelFormat() const {
    return static_cast<MNN::ModeFormat>(GetField<int8_t>(VT_MODELFORMAT, 0));
  }
  MNN::DataType type() const {
    return static_cast<MNN::DataType>(GetField<int32_t>(VT_TYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_INPUTQUANTIZEDPARAM) &&
           verifier.VerifyTable(inputQuantizedParam()) &&
           VerifyField<int8_t>(verifier, VT_MODE, 1) &&
           VerifyField<int8_t>(verifier, VT_MODELFORMAT, 1) &&
           VerifyField<int32_t>(verifier, VT_TYPE, 4) &&
           verifier.EndTable();
  }
};

struct DequantizeBuilder {
  typedef Dequantize Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_inputQuantizedParam(::flatbuffers::Offset<MNN::QuantizedParam> inputQuantizedParam) {
    fbb_.AddOffset(Dequantize::VT_INPUTQUANTIZEDPARAM, inputQuantizedParam);
  }
  void add_mode(MNN::QuantizeMode mode) {
    fbb_.AddElement<int8_t>(Dequantize::VT_MODE, static_cast<int8_t>(mode), 0);
  }
  void add_modelFormat(MNN::ModeFormat modelFormat) {
    fbb_.AddElement<int8_t>(Dequantize::VT_MODELFORMAT, static_cast<int8_t>(modelFormat), 0);
  }
  void add_type(MNN::DataType type) {
    fbb_.AddElement<int32_t>(Dequantize::VT_TYPE, static_cast<int32_t>(type), 0);
  }
  explicit DequantizeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Dequantize> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Dequantize>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Dequantize> CreateDequantize(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<MNN::QuantizedParam> inputQuantizedParam = 0,
    MNN::QuantizeMode mode = MNN::QuantizeMode_MIN_COMBINED,
    MNN::ModeFormat modelFormat = MNN::ModeFormat_TENSORFLOW,
    MNN::DataType type = MNN::DataType_DT_INVALID) {
  DequantizeBuilder builder_(_fbb);
  builder_.add_type(type);
  builder_.add_inputQuantizedParam(inputQuantizedParam);
  builder_.add_modelFormat(modelFormat);
  builder_.add_mode(mode);
  return builder_.Finish();
}

struct QuantizedAvgPool FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef QuantizedAvgPoolBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_KERNELX = 4,
    VT_KERNELY = 6,
    VT_MODELFORMAT = 8,
    VT_OUTPUTACTIVATIONMAX = 10,
    VT_OUTPUTACTIVATIONMIN = 12,
    VT_PADTYPE = 14,
    VT_PADX = 16,
    VT_PADY = 18,
    VT_STRIDEX = 20,
    VT_STRIDEY = 22,
    VT_TYPE = 24
  };
  int32_t kernelX() const {
    return GetField<int32_t>(VT_KERNELX, 0);
  }
  int32_t kernelY() const {
    return GetField<int32_t>(VT_KERNELY, 0);
  }
  MNN::ModeFormat modelFormat() const {
    return static_cast<MNN::ModeFormat>(GetField<int8_t>(VT_MODELFORMAT, 0));
  }
  int32_t outputActivationMax() const {
    return GetField<int32_t>(VT_OUTPUTACTIVATIONMAX, 0);
  }
  int32_t outputActivationMin() const {
    return GetField<int32_t>(VT_OUTPUTACTIVATIONMIN, 0);
  }
  MNN::PoolPadType padType() const {
    return static_cast<MNN::PoolPadType>(GetField<int8_t>(VT_PADTYPE, 0));
  }
  int32_t padX() const {
    return GetField<int32_t>(VT_PADX, 0);
  }
  int32_t padY() const {
    return GetField<int32_t>(VT_PADY, 0);
  }
  int32_t strideX() const {
    return GetField<int32_t>(VT_STRIDEX, 0);
  }
  int32_t strideY() const {
    return GetField<int32_t>(VT_STRIDEY, 0);
  }
  MNN::DataType type() const {
    return static_cast<MNN::DataType>(GetField<int32_t>(VT_TYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_KERNELX, 4) &&
           VerifyField<int32_t>(verifier, VT_KERNELY, 4) &&
           VerifyField<int8_t>(verifier, VT_MODELFORMAT, 1) &&
           VerifyField<int32_t>(verifier, VT_OUTPUTACTIVATIONMAX, 4) &&
           VerifyField<int32_t>(verifier, VT_OUTPUTACTIVATIONMIN, 4) &&
           VerifyField<int8_t>(verifier, VT_PADTYPE, 1) &&
           VerifyField<int32_t>(verifier, VT_PADX, 4) &&
           VerifyField<int32_t>(verifier, VT_PADY, 4) &&
           VerifyField<int32_t>(verifier, VT_STRIDEX, 4) &&
           VerifyField<int32_t>(verifier, VT_STRIDEY, 4) &&
           VerifyField<int32_t>(verifier, VT_TYPE, 4) &&
           verifier.EndTable();
  }
};

struct QuantizedAvgPoolBuilder {
  typedef QuantizedAvgPool Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_kernelX(int32_t kernelX) {
    fbb_.AddElement<int32_t>(QuantizedAvgPool::VT_KERNELX, kernelX, 0);
  }
  void add_kernelY(int32_t kernelY) {
    fbb_.AddElement<int32_t>(QuantizedAvgPool::VT_KERNELY, kernelY, 0);
  }
  void add_modelFormat(MNN::ModeFormat modelFormat) {
    fbb_.AddElement<int8_t>(QuantizedAvgPool::VT_MODELFORMAT, static_cast<int8_t>(modelFormat), 0);
  }
  void add_outputActivationMax(int32_t outputActivationMax) {
    fbb_.AddElement<int32_t>(QuantizedAvgPool::VT_OUTPUTACTIVATIONMAX, outputActivationMax, 0);
  }
  void add_outputActivationMin(int32_t outputActivationMin) {
    fbb_.AddElement<int32_t>(QuantizedAvgPool::VT_OUTPUTACTIVATIONMIN, outputActivationMin, 0);
  }
  void add_padType(MNN::PoolPadType padType) {
    fbb_.AddElement<int8_t>(QuantizedAvgPool::VT_PADTYPE, static_cast<int8_t>(padType), 0);
  }
  void add_padX(int32_t padX) {
    fbb_.AddElement<int32_t>(QuantizedAvgPool::VT_PADX, padX, 0);
  }
  void add_padY(int32_t padY) {
    fbb_.AddElement<int32_t>(QuantizedAvgPool::VT_PADY, padY, 0);
  }
  void add_strideX(int32_t strideX) {
    fbb_.AddElement<int32_t>(QuantizedAvgPool::VT_STRIDEX, strideX, 0);
  }
  void add_strideY(int32_t strideY) {
    fbb_.AddElement<int32_t>(QuantizedAvgPool::VT_STRIDEY, strideY, 0);
  }
  void add_type(MNN::DataType type) {
    fbb_.AddElement<int32_t>(QuantizedAvgPool::VT_TYPE, static_cast<int32_t>(type), 0);
  }
  explicit QuantizedAvgPoolBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<QuantizedAvgPool> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<QuantizedAvgPool>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<QuantizedAvgPool> CreateQuantizedAvgPool(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int32_t kernelX = 0,
    int32_t kernelY = 0,
    MNN::ModeFormat modelFormat = MNN::ModeFormat_TENSORFLOW,
    int32_t outputActivationMax = 0,
    int32_t outputActivationMin = 0,
    MNN::PoolPadType padType = MNN::PoolPadType_CAFFE,
    int32_t padX = 0,
    int32_t padY = 0,
    int32_t strideX = 0,
    int32_t strideY = 0,
    MNN::DataType type = MNN::DataType_DT_INVALID) {
  QuantizedAvgPoolBuilder builder_(_fbb);
  builder_.add_type(type);
  builder_.add_strideY(strideY);
  builder_.add_strideX(strideX);
  builder_.add_padY(padY);
  builder_.add_padX(padX);
  builder_.add_outputActivationMin(outputActivationMin);
  builder_.add_outputActivationMax(outputActivationMax);
  builder_.add_kernelY(kernelY);
  builder_.add_kernelX(kernelX);
  builder_.add_padType(padType);
  builder_.add_modelFormat(modelFormat);
  return builder_.Finish();
}

struct QuantizedBiasAdd FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef QuantizedBiasAddBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BIAS = 4,
    VT_INPUTTYPE = 6,
    VT_MAX = 8,
    VT_MIN = 10,
    VT_OUTPUTTYPE = 12
  };
  const ::flatbuffers::Vector<int32_t> *bias() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_BIAS);
  }
  MNN::DataType inputType() const {
    return static_cast<MNN::DataType>(GetField<int32_t>(VT_INPUTTYPE, 0));
  }
  int32_t max() const {
    return GetField<int32_t>(VT_MAX, 0);
  }
  int32_t min() const {
    return GetField<int32_t>(VT_MIN, 0);
  }
  MNN::DataType outputType() const {
    return static_cast<MNN::DataType>(GetField<int32_t>(VT_OUTPUTTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_BIAS) &&
           verifier.VerifyVector(bias()) &&
           VerifyField<int32_t>(verifier, VT_INPUTTYPE, 4) &&
           VerifyField<int32_t>(verifier, VT_MAX, 4) &&
           VerifyField<int32_t>(verifier, VT_MIN, 4) &&
           VerifyField<int32_t>(verifier, VT_OUTPUTTYPE, 4) &&
           verifier.EndTable();
  }
};

struct QuantizedBiasAddBuilder {
  typedef QuantizedBiasAdd Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_bias(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> bias) {
    fbb_.AddOffset(QuantizedBiasAdd::VT_BIAS, bias);
  }
  void add_inputType(MNN::DataType inputType) {
    fbb_.AddElement<int32_t>(QuantizedBiasAdd::VT_INPUTTYPE, static_cast<int32_t>(inputType), 0);
  }
  void add_max(int32_t max) {
    fbb_.AddElement<int32_t>(QuantizedBiasAdd::VT_MAX, max, 0);
  }
  void add_min(int32_t min) {
    fbb_.AddElement<int32_t>(QuantizedBiasAdd::VT_MIN, min, 0);
  }
  void add_outputType(MNN::DataType outputType) {
    fbb_.AddElement<int32_t>(QuantizedBiasAdd::VT_OUTPUTTYPE, static_cast<int32_t>(outputType), 0);
  }
  explicit QuantizedBiasAddBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<QuantizedBiasAdd> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<QuantizedBiasAdd>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<QuantizedBiasAdd> CreateQuantizedBiasAdd(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> bias = 0,
    MNN::DataType inputType = MNN::DataType_DT_INVALID,
    int32_t max = 0,
    int32_t min = 0,
    MNN::DataType outputType = MNN::DataType_DT_INVALID) {
  QuantizedBiasAddBuilder builder_(_fbb);
  builder_.add_outputType(outputType);
  builder_.add_min(min);
  builder_.add_max(max);
  builder_.add_inputType(inputType);
  builder_.add_bias(bias);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<QuantizedBiasAdd> CreateQuantizedBiasAddDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int32_t> *bias = nullptr,
    MNN::DataType inputType = MNN::DataType_DT_INVALID,
    int32_t max = 0,
    int32_t min = 0,
    MNN::DataType outputType = MNN::DataType_DT_INVALID) {
  auto bias__ = bias ? _fbb.CreateVector<int32_t>(*bias) : 0;
  return MNN::CreateQuantizedBiasAdd(
      _fbb,
      bias__,
      inputType,
      max,
      min,
      outputType);
}

struct QuantizedConcat FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef QuantizedConcatBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ACTIVATIONTYPE = 4,
    VT_AXIS = 6,
    VT_INPUTSCALE = 8,
    VT_INPUTZEROPOINT = 10,
    VT_OUTPUTQUANTIZEDPARAM = 12
  };
  MNN::FusedActivation activationType() const {
    return static_cast<MNN::FusedActivation>(GetField<int8_t>(VT_ACTIVATIONTYPE, 0));
  }
  int32_t axis() const {
    return GetField<int32_t>(VT_AXIS, 0);
  }
  const ::flatbuffers::Vector<float> *inputScale() const {
    return GetPointer<const ::flatbuffers::Vector<float> *>(VT_INPUTSCALE);
  }
  const ::flatbuffers::Vector<int32_t> *inputZeroPoint() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_INPUTZEROPOINT);
  }
  const MNN::QuantizedParam *outputQuantizedParam() const {
    return GetPointer<const MNN::QuantizedParam *>(VT_OUTPUTQUANTIZEDPARAM);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int8_t>(verifier, VT_ACTIVATIONTYPE, 1) &&
           VerifyField<int32_t>(verifier, VT_AXIS, 4) &&
           VerifyOffset(verifier, VT_INPUTSCALE) &&
           verifier.VerifyVector(inputScale()) &&
           VerifyOffset(verifier, VT_INPUTZEROPOINT) &&
           verifier.VerifyVector(inputZeroPoint()) &&
           VerifyOffset(verifier, VT_OUTPUTQUANTIZEDPARAM) &&
           verifier.VerifyTable(outputQuantizedParam()) &&
           verifier.EndTable();
  }
};

struct QuantizedConcatBuilder {
  typedef QuantizedConcat Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_activationType(MNN::FusedActivation activationType) {
    fbb_.AddElement<int8_t>(QuantizedConcat::VT_ACTIVATIONTYPE, static_cast<int8_t>(activationType), 0);
  }
  void add_axis(int32_t axis) {
    fbb_.AddElement<int32_t>(QuantizedConcat::VT_AXIS, axis, 0);
  }
  void add_inputScale(::flatbuffers::Offset<::flatbuffers::Vector<float>> inputScale) {
    fbb_.AddOffset(QuantizedConcat::VT_INPUTSCALE, inputScale);
  }
  void add_inputZeroPoint(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> inputZeroPoint) {
    fbb_.AddOffset(QuantizedConcat::VT_INPUTZEROPOINT, inputZeroPoint);
  }
  void add_outputQuantizedParam(::flatbuffers::Offset<MNN::QuantizedParam> outputQuantizedParam) {
    fbb_.AddOffset(QuantizedConcat::VT_OUTPUTQUANTIZEDPARAM, outputQuantizedParam);
  }
  explicit QuantizedConcatBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<QuantizedConcat> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<QuantizedConcat>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<QuantizedConcat> CreateQuantizedConcat(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    MNN::FusedActivation activationType = MNN::FusedActivation_kTfLiteActNone,
    int32_t axis = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<float>> inputScale = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> inputZeroPoint = 0,
    ::flatbuffers::Offset<MNN::QuantizedParam> outputQuantizedParam = 0) {
  QuantizedConcatBuilder builder_(_fbb);
  builder_.add_outputQuantizedParam(outputQuantizedParam);
  builder_.add_inputZeroPoint(inputZeroPoint);
  builder_.add_inputScale(inputScale);
  builder_.add_axis(axis);
  builder_.add_activationType(activationType);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<QuantizedConcat> CreateQuantizedConcatDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    MNN::FusedActivation activationType = MNN::FusedActivation_kTfLiteActNone,
    int32_t axis = 0,
    const std::vector<float> *inputScale = nullptr,
    const std::vector<int32_t> *inputZeroPoint = nullptr,
    ::flatbuffers::Offset<MNN::QuantizedParam> outputQuantizedParam = 0) {
  auto inputScale__ = inputScale ? _fbb.CreateVector<float>(*inputScale) : 0;
  auto inputZeroPoint__ = inputZeroPoint ? _fbb.CreateVector<int32_t>(*inputZeroPoint) : 0;
  return MNN::CreateQuantizedConcat(
      _fbb,
      activationType,
      axis,
      inputScale__,
      inputZeroPoint__,
      outputQuantizedParam);
}

struct QuantizedLogistic FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef QuantizedLogisticBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INPUTQUANTIZEDPARAM = 4,
    VT_OUTPUTQUANTIZEDPARAM = 6
  };
  const MNN::QuantizedParam *inputQuantizedParam() const {
    return GetPointer<const MNN::QuantizedParam *>(VT_INPUTQUANTIZEDPARAM);
  }
  const MNN::QuantizedParam *outputQuantizedParam() const {
    return GetPointer<const MNN::QuantizedParam *>(VT_OUTPUTQUANTIZEDPARAM);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_INPUTQUANTIZEDPARAM) &&
           verifier.VerifyTable(inputQuantizedParam()) &&
           VerifyOffset(verifier, VT_OUTPUTQUANTIZEDPARAM) &&
           verifier.VerifyTable(outputQuantizedParam()) &&
           verifier.EndTable();
  }
};

struct QuantizedLogisticBuilder {
  typedef QuantizedLogistic Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_inputQuantizedParam(::flatbuffers::Offset<MNN::QuantizedParam> inputQuantizedParam) {
    fbb_.AddOffset(QuantizedLogistic::VT_INPUTQUANTIZEDPARAM, inputQuantizedParam);
  }
  void add_outputQuantizedParam(::flatbuffers::Offset<MNN::QuantizedParam> outputQuantizedParam) {
    fbb_.AddOffset(QuantizedLogistic::VT_OUTPUTQUANTIZEDPARAM, outputQuantizedParam);
  }
  explicit QuantizedLogisticBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<QuantizedLogistic> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<QuantizedLogistic>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<QuantizedLogistic> CreateQuantizedLogistic(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<MNN::QuantizedParam> inputQuantizedParam = 0,
    ::flatbuffers::Offset<MNN::QuantizedParam> outputQuantizedParam = 0) {
  QuantizedLogisticBuilder builder_(_fbb);
  builder_.add_outputQuantizedParam(outputQuantizedParam);
  builder_.add_inputQuantizedParam(inputQuantizedParam);
  return builder_.Finish();
}

struct QuantizedMatMul FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef QuantizedMatMulBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TRANSPOSEA = 4,
    VT_TRANSPOSEB = 6
  };
  bool transposeA() const {
    return GetField<uint8_t>(VT_TRANSPOSEA, 0) != 0;
  }
  bool transposeB() const {
    return GetField<uint8_t>(VT_TRANSPOSEB, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_TRANSPOSEA, 1) &&
           VerifyField<uint8_t>(verifier, VT_TRANSPOSEB, 1) &&
           verifier.EndTable();
  }
};

struct QuantizedMatMulBuilder {
  typedef QuantizedMatMul Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_transposeA(bool transposeA) {
    fbb_.AddElement<uint8_t>(QuantizedMatMul::VT_TRANSPOSEA, static_cast<uint8_t>(transposeA), 0);
  }
  void add_transposeB(bool transposeB) {
    fbb_.AddElement<uint8_t>(QuantizedMatMul::VT_TRANSPOSEB, static_cast<uint8_t>(transposeB), 0);
  }
  explicit QuantizedMatMulBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<QuantizedMatMul> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<QuantizedMatMul>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<QuantizedMatMul> CreateQuantizedMatMul(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool transposeA = false,
    bool transposeB = false) {
  QuantizedMatMulBuilder builder_(_fbb);
  builder_.add_transposeB(transposeB);
  builder_.add_transposeA(transposeA);
  return builder_.Finish();
}

struct QuantizedMaxPool FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef QuantizedMaxPoolBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_KERNELX = 4,
    VT_KERNELY = 6,
    VT_MODELFORMAT = 8,
    VT_OUTPUTACTIVATIONMAX = 10,
    VT_OUTPUTACTIVATIONMIN = 12,
    VT_PADTYPE = 14,
    VT_PADX = 16,
    VT_PADY = 18,
    VT_STRIDEX = 20,
    VT_STRIDEY = 22,
    VT_TYPE = 24
  };
  int32_t kernelX() const {
    return GetField<int32_t>(VT_KERNELX, 0);
  }
  int32_t kernelY() const {
    return GetField<int32_t>(VT_KERNELY, 0);
  }
  MNN::ModeFormat modelFormat() const {
    return static_cast<MNN::ModeFormat>(GetField<int8_t>(VT_MODELFORMAT, 0));
  }
  int32_t outputActivationMax() const {
    return GetField<int32_t>(VT_OUTPUTACTIVATIONMAX, 0);
  }
  int32_t outputActivationMin() const {
    return GetField<int32_t>(VT_OUTPUTACTIVATIONMIN, 0);
  }
  MNN::PoolPadType padType() const {
    return static_cast<MNN::PoolPadType>(GetField<int8_t>(VT_PADTYPE, 0));
  }
  int32_t padX() const {
    return GetField<int32_t>(VT_PADX, 0);
  }
  int32_t padY() const {
    return GetField<int32_t>(VT_PADY, 0);
  }
  int32_t strideX() const {
    return GetField<int32_t>(VT_STRIDEX, 0);
  }
  int32_t strideY() const {
    return GetField<int32_t>(VT_STRIDEY, 0);
  }
  MNN::DataType type() const {
    return static_cast<MNN::DataType>(GetField<int32_t>(VT_TYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_KERNELX, 4) &&
           VerifyField<int32_t>(verifier, VT_KERNELY, 4) &&
           VerifyField<int8_t>(verifier, VT_MODELFORMAT, 1) &&
           VerifyField<int32_t>(verifier, VT_OUTPUTACTIVATIONMAX, 4) &&
           VerifyField<int32_t>(verifier, VT_OUTPUTACTIVATIONMIN, 4) &&
           VerifyField<int8_t>(verifier, VT_PADTYPE, 1) &&
           VerifyField<int32_t>(verifier, VT_PADX, 4) &&
           VerifyField<int32_t>(verifier, VT_PADY, 4) &&
           VerifyField<int32_t>(verifier, VT_STRIDEX, 4) &&
           VerifyField<int32_t>(verifier, VT_STRIDEY, 4) &&
           VerifyField<int32_t>(verifier, VT_TYPE, 4) &&
           verifier.EndTable();
  }
};

struct QuantizedMaxPoolBuilder {
  typedef QuantizedMaxPool Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_kernelX(int32_t kernelX) {
    fbb_.AddElement<int32_t>(QuantizedMaxPool::VT_KERNELX, kernelX, 0);
  }
  void add_kernelY(int32_t kernelY) {
    fbb_.AddElement<int32_t>(QuantizedMaxPool::VT_KERNELY, kernelY, 0);
  }
  void add_modelFormat(MNN::ModeFormat modelFormat) {
    fbb_.AddElement<int8_t>(QuantizedMaxPool::VT_MODELFORMAT, static_cast<int8_t>(modelFormat), 0);
  }
  void add_outputActivationMax(int32_t outputActivationMax) {
    fbb_.AddElement<int32_t>(QuantizedMaxPool::VT_OUTPUTACTIVATIONMAX, outputActivationMax, 0);
  }
  void add_outputActivationMin(int32_t outputActivationMin) {
    fbb_.AddElement<int32_t>(QuantizedMaxPool::VT_OUTPUTACTIVATIONMIN, outputActivationMin, 0);
  }
  void add_padType(MNN::PoolPadType padType) {
    fbb_.AddElement<int8_t>(QuantizedMaxPool::VT_PADTYPE, static_cast<int8_t>(padType), 0);
  }
  void add_padX(int32_t padX) {
    fbb_.AddElement<int32_t>(QuantizedMaxPool::VT_PADX, padX, 0);
  }
  void add_padY(int32_t padY) {
    fbb_.AddElement<int32_t>(QuantizedMaxPool::VT_PADY, padY, 0);
  }
  void add_strideX(int32_t strideX) {
    fbb_.AddElement<int32_t>(QuantizedMaxPool::VT_STRIDEX, strideX, 0);
  }
  void add_strideY(int32_t strideY) {
    fbb_.AddElement<int32_t>(QuantizedMaxPool::VT_STRIDEY, strideY, 0);
  }
  void add_type(MNN::DataType type) {
    fbb_.AddElement<int32_t>(QuantizedMaxPool::VT_TYPE, static_cast<int32_t>(type), 0);
  }
  explicit QuantizedMaxPoolBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<QuantizedMaxPool> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<QuantizedMaxPool>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<QuantizedMaxPool> CreateQuantizedMaxPool(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int32_t kernelX = 0,
    int32_t kernelY = 0,
    MNN::ModeFormat modelFormat = MNN::ModeFormat_TENSORFLOW,
    int32_t outputActivationMax = 0,
    int32_t outputActivationMin = 0,
    MNN::PoolPadType padType = MNN::PoolPadType_CAFFE,
    int32_t padX = 0,
    int32_t padY = 0,
    int32_t strideX = 0,
    int32_t strideY = 0,
    MNN::DataType type = MNN::DataType_DT_INVALID) {
  QuantizedMaxPoolBuilder builder_(_fbb);
  builder_.add_type(type);
  builder_.add_strideY(strideY);
  builder_.add_strideX(strideX);
  builder_.add_padY(padY);
  builder_.add_padX(padX);
  builder_.add_outputActivationMin(outputActivationMin);
  builder_.add_outputActivationMax(outputActivationMax);
  builder_.add_kernelY(kernelY);
  builder_.add_kernelX(kernelX);
  builder_.add_padType(padType);
  builder_.add_modelFormat(modelFormat);
  return builder_.Finish();
}

struct QuantizedRelu FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef QuantizedReluBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TYPE = 4
  };
  MNN::DataType type() const {
    return static_cast<MNN::DataType>(GetField<int32_t>(VT_TYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_TYPE, 4) &&
           verifier.EndTable();
  }
};

struct QuantizedReluBuilder {
  typedef QuantizedRelu Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_type(MNN::DataType type) {
    fbb_.AddElement<int32_t>(QuantizedRelu::VT_TYPE, static_cast<int32_t>(type), 0);
  }
  explicit QuantizedReluBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<QuantizedRelu> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<QuantizedRelu>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<QuantizedRelu> CreateQuantizedRelu(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    MNN::DataType type = MNN::DataType_DT_INVALID) {
  QuantizedReluBuilder builder_(_fbb);
  builder_.add_type(type);
  return builder_.Finish();
}

struct QuantizedRelu6 FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef QuantizedRelu6Builder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TYPE = 4
  };
  MNN::DataType type() const {
    return static_cast<MNN::DataType>(GetField<int32_t>(VT_TYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_TYPE, 4) &&
           verifier.EndTable();
  }
};

struct QuantizedRelu6Builder {
  typedef QuantizedRelu6 Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_type(MNN::DataType type) {
    fbb_.AddElement<int32_t>(QuantizedRelu6::VT_TYPE, static_cast<int32_t>(type), 0);
  }
  explicit QuantizedRelu6Builder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<QuantizedRelu6> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<QuantizedRelu6>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<QuantizedRelu6> CreateQuantizedRelu6(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    MNN::DataType type = MNN::DataType_DT_INVALID) {
  QuantizedRelu6Builder builder_(_fbb);
  builder_.add_type(type);
  return builder_.Finish();
}

struct QuantizedReshape FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef QuantizedReshapeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIMS = 4,
    VT_MODELFORMAT = 6
  };
  const ::flatbuffers::Vector<int32_t> *dims() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_DIMS);
  }
  MNN::ModeFormat modelFormat() const {
    return static_cast<MNN::ModeFormat>(GetField<int8_t>(VT_MODELFORMAT, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_DIMS) &&
           verifier.VerifyVector(dims()) &&
           VerifyField<int8_t>(verifier, VT_MODELFORMAT, 1) &&
           verifier.EndTable();
  }
};

struct QuantizedReshapeBuilder {
  typedef QuantizedReshape Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dims(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> dims) {
    fbb_.AddOffset(QuantizedReshape::VT_DIMS, dims);
  }
  void add_modelFormat(MNN::ModeFormat modelFormat) {
    fbb_.AddElement<int8_t>(QuantizedReshape::VT_MODELFORMAT, static_cast<int8_t>(modelFormat), 0);
  }
  explicit QuantizedReshapeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<QuantizedReshape> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<QuantizedReshape>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<QuantizedReshape> CreateQuantizedReshape(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> dims = 0,
    MNN::ModeFormat modelFormat = MNN::ModeFormat_TENSORFLOW) {
  QuantizedReshapeBuilder builder_(_fbb);
  builder_.add_dims(dims);
  builder_.add_modelFormat(modelFormat);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<QuantizedReshape> CreateQuantizedReshapeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int32_t> *dims = nullptr,
    MNN::ModeFormat modelFormat = MNN::ModeFormat_TENSORFLOW) {
  auto dims__ = dims ? _fbb.CreateVector<int32_t>(*dims) : 0;
  return MNN::CreateQuantizedReshape(
      _fbb,
      dims__,
      modelFormat);
}

struct QuantizedSoftmax FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef QuantizedSoftmaxBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BETA = 4,
    VT_INPUTSCALE = 6
  };
  float beta() const {
    return GetField<float>(VT_BETA, 0.0f);
  }
  float inputScale() const {
    return GetField<float>(VT_INPUTSCALE, 0.0f);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<float>(verifier, VT_BETA, 4) &&
           VerifyField<float>(verifier, VT_INPUTSCALE, 4) &&
           verifier.EndTable();
  }
};

struct QuantizedSoftmaxBuilder {
  typedef QuantizedSoftmax Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_beta(float beta) {
    fbb_.AddElement<float>(QuantizedSoftmax::VT_BETA, beta, 0.0f);
  }
  void add_inputScale(float inputScale) {
    fbb_.AddElement<float>(QuantizedSoftmax::VT_INPUTSCALE, inputScale, 0.0f);
  }
  explicit QuantizedSoftmaxBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<QuantizedSoftmax> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<QuantizedSoftmax>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<QuantizedSoftmax> CreateQuantizedSoftmax(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    float beta = 0.0f,
    float inputScale = 0.0f) {
  QuantizedSoftmaxBuilder builder_(_fbb);
  builder_.add_inputScale(inputScale);
  builder_.add_beta(beta);
  return builder_.Finish();
}

struct QuantizeV2 FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef QuantizeV2Builder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TYPE = 4,
    VT_MODE = 6,
    VT_ROUNDMODE = 8
  };
  MNN::DataType type() const {
    return static_cast<MNN::DataType>(GetField<int32_t>(VT_TYPE, 0));
  }
  MNN::QuantizeMode mode() const {
    return static_cast<MNN::QuantizeMode>(GetField<int8_t>(VT_MODE, 0));
  }
  MNN::QuantizeRoundMode roundMode() const {
    return static_cast<MNN::QuantizeRoundMode>(GetField<int8_t>(VT_ROUNDMODE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_TYPE, 4) &&
           VerifyField<int8_t>(verifier, VT_MODE, 1) &&
           VerifyField<int8_t>(verifier, VT_ROUNDMODE, 1) &&
           verifier.EndTable();
  }
};

struct QuantizeV2Builder {
  typedef QuantizeV2 Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_type(MNN::DataType type) {
    fbb_.AddElement<int32_t>(QuantizeV2::VT_TYPE, static_cast<int32_t>(type), 0);
  }
  void add_mode(MNN::QuantizeMode mode) {
    fbb_.AddElement<int8_t>(QuantizeV2::VT_MODE, static_cast<int8_t>(mode), 0);
  }
  void add_roundMode(MNN::QuantizeRoundMode roundMode) {
    fbb_.AddElement<int8_t>(QuantizeV2::VT_ROUNDMODE, static_cast<int8_t>(roundMode), 0);
  }
  explicit QuantizeV2Builder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<QuantizeV2> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<QuantizeV2>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<QuantizeV2> CreateQuantizeV2(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    MNN::DataType type = MNN::DataType_DT_INVALID,
    MNN::QuantizeMode mode = MNN::QuantizeMode_MIN_COMBINED,
    MNN::QuantizeRoundMode roundMode = MNN::QuantizeRoundMode_HALF_AWAY_FROM_ZERO) {
  QuantizeV2Builder builder_(_fbb);
  builder_.add_type(type);
  builder_.add_roundMode(roundMode);
  builder_.add_mode(mode);
  return builder_.Finish();
}

struct RequantizationRange FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef RequantizationRangeBuilder Builder;
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           verifier.EndTable();
  }
};

struct RequantizationRangeBuilder {
  typedef RequantizationRange Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  explicit RequantizationRangeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<RequantizationRange> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<RequantizationRange>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<RequantizationRange> CreateRequantizationRange(
    ::flatbuffers::FlatBufferBuilder &_fbb) {
  RequantizationRangeBuilder builder_(_fbb);
  return builder_.Finish();
}

struct Requantize FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef RequantizeBuilder Builder;
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           verifier.EndTable();
  }
};

struct RequantizeBuilder {
  typedef Requantize Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  explicit RequantizeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Requantize> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Requantize>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Requantize> CreateRequantize(
    ::flatbuffers::FlatBufferBuilder &_fbb) {
  RequantizeBuilder builder_(_fbb);
  return builder_.Finish();
}

struct TfQuantizedConv2D FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TfQuantizedConv2DBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BIAS = 4,
    VT_BIASFLAG = 6,
    VT_COMMON = 8,
    VT_WEIGHT = 10,
    VT_ACTIVATIONTYPE = 12,
    VT_MULTIPLIER = 14,
    VT_OUTMAX = 16,
    VT_OUTMIN = 18,
    VT_SHIFT = 20,
    VT_BIASQUANTIZEDPARAM = 22,
    VT_DEPTHMULTIPLIER = 24,
    VT_FILTERQUANTIZEDPARAM = 26,
    VT_INPUTQUANTIZEDPARAM = 28,
    VT_MODELFORMAT = 30,
    VT_OUTPUTQUANTIZEDPARAM = 32
  };
  const ::flatbuffers::Vector<int32_t> *bias() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_BIAS);
  }
  bool biasflag() const {
    return GetField<uint8_t>(VT_BIASFLAG, 0) != 0;
  }
  const MNN::Convolution2DCommon *common() const {
    return GetPointer<const MNN::Convolution2DCommon *>(VT_COMMON);
  }
  const ::flatbuffers::Vector<uint8_t> *weight() const {
    return GetPointer<const ::flatbuffers::Vector<uint8_t> *>(VT_WEIGHT);
  }
  MNN::FusedActivation activationType() const {
    return static_cast<MNN::FusedActivation>(GetField<int8_t>(VT_ACTIVATIONTYPE, 0));
  }
  int32_t multiplier() const {
    return GetField<int32_t>(VT_MULTIPLIER, 0);
  }
  int32_t outMax() const {
    return GetField<int32_t>(VT_OUTMAX, 0);
  }
  int32_t outMin() const {
    return GetField<int32_t>(VT_OUTMIN, 0);
  }
  int32_t shift() const {
    return GetField<int32_t>(VT_SHIFT, 0);
  }
  const MNN::QuantizedParam *biasQuantizedParam() const {
    return GetPointer<const MNN::QuantizedParam *>(VT_BIASQUANTIZEDPARAM);
  }
  int32_t depthMultiplier() const {
    return GetField<int32_t>(VT_DEPTHMULTIPLIER, 0);
  }
  const MNN::QuantizedParam *filterQuantizedParam() const {
    return GetPointer<const MNN::QuantizedParam *>(VT_FILTERQUANTIZEDPARAM);
  }
  const MNN::QuantizedParam *inputQuantizedParam() const {
    return GetPointer<const MNN::QuantizedParam *>(VT_INPUTQUANTIZEDPARAM);
  }
  MNN::ModeFormat modelFormat() const {
    return static_cast<MNN::ModeFormat>(GetField<int8_t>(VT_MODELFORMAT, 0));
  }
  const MNN::QuantizedParam *outputQuantizedParam() const {
    return GetPointer<const MNN::QuantizedParam *>(VT_OUTPUTQUANTIZEDPARAM);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_BIAS) &&
           verifier.VerifyVector(bias()) &&
           VerifyField<uint8_t>(verifier, VT_BIASFLAG, 1) &&
           VerifyOffset(verifier, VT_COMMON) &&
           verifier.VerifyTable(common()) &&
           VerifyOffset(verifier, VT_WEIGHT) &&
           verifier.VerifyVector(weight()) &&
           VerifyField<int8_t>(verifier, VT_ACTIVATIONTYPE, 1) &&
           VerifyField<int32_t>(verifier, VT_MULTIPLIER, 4) &&
           VerifyField<int32_t>(verifier, VT_OUTMAX, 4) &&
           VerifyField<int32_t>(verifier, VT_OUTMIN, 4) &&
           VerifyField<int32_t>(verifier, VT_SHIFT, 4) &&
           VerifyOffset(verifier, VT_BIASQUANTIZEDPARAM) &&
           verifier.VerifyTable(biasQuantizedParam()) &&
           VerifyField<int32_t>(verifier, VT_DEPTHMULTIPLIER, 4) &&
           VerifyOffset(verifier, VT_FILTERQUANTIZEDPARAM) &&
           verifier.VerifyTable(filterQuantizedParam()) &&
           VerifyOffset(verifier, VT_INPUTQUANTIZEDPARAM) &&
           verifier.VerifyTable(inputQuantizedParam()) &&
           VerifyField<int8_t>(verifier, VT_MODELFORMAT, 1) &&
           VerifyOffset(verifier, VT_OUTPUTQUANTIZEDPARAM) &&
           verifier.VerifyTable(outputQuantizedParam()) &&
           verifier.EndTable();
  }
};

struct TfQuantizedConv2DBuilder {
  typedef TfQuantizedConv2D Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_bias(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> bias) {
    fbb_.AddOffset(TfQuantizedConv2D::VT_BIAS, bias);
  }
  void add_biasflag(bool biasflag) {
    fbb_.AddElement<uint8_t>(TfQuantizedConv2D::VT_BIASFLAG, static_cast<uint8_t>(biasflag), 0);
  }
  void add_common(::flatbuffers::Offset<MNN::Convolution2DCommon> common) {
    fbb_.AddOffset(TfQuantizedConv2D::VT_COMMON, common);
  }
  void add_weight(::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> weight) {
    fbb_.AddOffset(TfQuantizedConv2D::VT_WEIGHT, weight);
  }
  void add_activationType(MNN::FusedActivation activationType) {
    fbb_.AddElement<int8_t>(TfQuantizedConv2D::VT_ACTIVATIONTYPE, static_cast<int8_t>(activationType), 0);
  }
  void add_multiplier(int32_t multiplier) {
    fbb_.AddElement<int32_t>(TfQuantizedConv2D::VT_MULTIPLIER, multiplier, 0);
  }
  void add_outMax(int32_t outMax) {
    fbb_.AddElement<int32_t>(TfQuantizedConv2D::VT_OUTMAX, outMax, 0);
  }
  void add_outMin(int32_t outMin) {
    fbb_.AddElement<int32_t>(TfQuantizedConv2D::VT_OUTMIN, outMin, 0);
  }
  void add_shift(int32_t shift) {
    fbb_.AddElement<int32_t>(TfQuantizedConv2D::VT_SHIFT, shift, 0);
  }
  void add_biasQuantizedParam(::flatbuffers::Offset<MNN::QuantizedParam> biasQuantizedParam) {
    fbb_.AddOffset(TfQuantizedConv2D::VT_BIASQUANTIZEDPARAM, biasQuantizedParam);
  }
  void add_depthMultiplier(int32_t depthMultiplier) {
    fbb_.AddElement<int32_t>(TfQuantizedConv2D::VT_DEPTHMULTIPLIER, depthMultiplier, 0);
  }
  void add_filterQuantizedParam(::flatbuffers::Offset<MNN::QuantizedParam> filterQuantizedParam) {
    fbb_.AddOffset(TfQuantizedConv2D::VT_FILTERQUANTIZEDPARAM, filterQuantizedParam);
  }
  void add_inputQuantizedParam(::flatbuffers::Offset<MNN::QuantizedParam> inputQuantizedParam) {
    fbb_.AddOffset(TfQuantizedConv2D::VT_INPUTQUANTIZEDPARAM, inputQuantizedParam);
  }
  void add_modelFormat(MNN::ModeFormat modelFormat) {
    fbb_.AddElement<int8_t>(TfQuantizedConv2D::VT_MODELFORMAT, static_cast<int8_t>(modelFormat), 0);
  }
  void add_outputQuantizedParam(::flatbuffers::Offset<MNN::QuantizedParam> outputQuantizedParam) {
    fbb_.AddOffset(TfQuantizedConv2D::VT_OUTPUTQUANTIZEDPARAM, outputQuantizedParam);
  }
  explicit TfQuantizedConv2DBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TfQuantizedConv2D> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TfQuantizedConv2D>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TfQuantizedConv2D> CreateTfQuantizedConv2D(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> bias = 0,
    bool biasflag = false,
    ::flatbuffers::Offset<MNN::Convolution2DCommon> common = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> weight = 0,
    MNN::FusedActivation activationType = MNN::FusedActivation_kTfLiteActNone,
    int32_t multiplier = 0,
    int32_t outMax = 0,
    int32_t outMin = 0,
    int32_t shift = 0,
    ::flatbuffers::Offset<MNN::QuantizedParam> biasQuantizedParam = 0,
    int32_t depthMultiplier = 0,
    ::flatbuffers::Offset<MNN::QuantizedParam> filterQuantizedParam = 0,
    ::flatbuffers::Offset<MNN::QuantizedParam> inputQuantizedParam = 0,
    MNN::ModeFormat modelFormat = MNN::ModeFormat_TENSORFLOW,
    ::flatbuffers::Offset<MNN::QuantizedParam> outputQuantizedParam = 0) {
  TfQuantizedConv2DBuilder builder_(_fbb);
  builder_.add_outputQuantizedParam(outputQuantizedParam);
  builder_.add_inputQuantizedParam(inputQuantizedParam);
  builder_.add_filterQuantizedParam(filterQuantizedParam);
  builder_.add_depthMultiplier(depthMultiplier);
  builder_.add_biasQuantizedParam(biasQuantizedParam);
  builder_.add_shift(shift);
  builder_.add_outMin(outMin);
  builder_.add_outMax(outMax);
  builder_.add_multiplier(multiplier);
  builder_.add_weight(weight);
  builder_.add_common(common);
  builder_.add_bias(bias);
  builder_.add_modelFormat(modelFormat);
  builder_.add_activationType(activationType);
  builder_.add_biasflag(biasflag);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TfQuantizedConv2D> CreateTfQuantizedConv2DDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int32_t> *bias = nullptr,
    bool biasflag = false,
    ::flatbuffers::Offset<MNN::Convolution2DCommon> common = 0,
    const std::vector<uint8_t> *weight = nullptr,
    MNN::FusedActivation activationType = MNN::FusedActivation_kTfLiteActNone,
    int32_t multiplier = 0,
    int32_t outMax = 0,
    int32_t outMin = 0,
    int32_t shift = 0,
    ::flatbuffers::Offset<MNN::QuantizedParam> biasQuantizedParam = 0,
    int32_t depthMultiplier = 0,
    ::flatbuffers::Offset<MNN::QuantizedParam> filterQuantizedParam = 0,
    ::flatbuffers::Offset<MNN::QuantizedParam> inputQuantizedParam = 0,
    MNN::ModeFormat modelFormat = MNN::ModeFormat_TENSORFLOW,
    ::flatbuffers::Offset<MNN::QuantizedParam> outputQuantizedParam = 0) {
  auto bias__ = bias ? _fbb.CreateVector<int32_t>(*bias) : 0;
  auto weight__ = weight ? _fbb.CreateVector<uint8_t>(*weight) : 0;
  return MNN::CreateTfQuantizedConv2D(
      _fbb,
      bias__,
      biasflag,
      common,
      weight__,
      activationType,
      multiplier,
      outMax,
      outMin,
      shift,
      biasQuantizedParam,
      depthMultiplier,
      filterQuantizedParam,
      inputQuantizedParam,
      modelFormat,
      outputQuantizedParam);
}

}  // namespace MNN

#endif  // FLATBUFFERS_GENERATED_TFQUANTIZEOP_MNN_H_
